{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    sentiment = blob.sentiment.polarity  # Retourne une valeur entre -1 (négatif) et 1 (positif)\n",
    "    \n",
    "    if sentiment > 0:\n",
    "        return 'Positif'\n",
    "    elif sentiment < 0:\n",
    "        return 'Négatif'\n",
    "    else:\n",
    "        return 'Neutre'\n",
    "\n",
    "# Fonction pour identifier le sujet avec Latent Dirichlet Allocation (LDA)\n",
    "def identify_topics(texts, n_topics=1, n_top_words=5):\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    dtm = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=0)\n",
    "    lda.fit(dtm)\n",
    "    \n",
    "    topics = []\n",
    "    for idx, topic in enumerate(lda.components_):\n",
    "        terms = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        topics.extend(terms)\n",
    "    return topics\n",
    "\n",
    "def scrape_news(topics, websites):\n",
    "    articles = []  \n",
    "    for website in websites:\n",
    "        response = requests.get(website)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # for link in soup.find_all('a', href=True):\n",
    "        #     link_text = link.text.strip().lower()\n",
    "\n",
    "\n",
    "        #     if all(topic.lower() in link_text for topic in topics):\n",
    "        #         articles.append({'title': link.text.strip(), 'url': link['href']})\n",
    "\n",
    "    return soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contenu du tweet :  The earth is flat\n",
      "Sentiment détecté :  Négatif\n",
      "Sujets identifiés :\n",
      "flat\n",
      "earth\n",
      "Websites :  ['https://www.wikipedia.org/wiki/earth']\n",
      "Articles trouvés :\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 24\u001b[0m\n\u001b[0;32m     19\u001b[0m     articles \u001b[38;5;241m=\u001b[39m scrape_news(topic[\u001b[38;5;241m1\u001b[39m], websites)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArticles trouvés :\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28mprint\u001b[39m(articles\u001b[38;5;241m.\u001b[39mtext[\u001b[43marticles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(topics[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m50\u001b[39m : articles\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mfind(topics[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m50\u001b[39m])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "tweet_content = \"The earth is flat\"\n",
    "\n",
    "print(\"Contenu du tweet : \", tweet_content)\n",
    "\n",
    "# Analyse des sentiments\n",
    "sentiment = analyze_sentiment(tweet_content)\n",
    "print(\"Sentiment détecté : \", sentiment)\n",
    "\n",
    "# Identification du sujet\n",
    "topics = identify_topics([tweet_content])\n",
    "print(\"Sujets identifiés :\")\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "\n",
    "websites = ['https://www.wikipedia.org/wiki/' + topics[1]]\n",
    "#websites = ['https://www.bbc.com/news', 'https://www.cnn.com', 'https://www.wikipedia.org/', 'https://www.lemonde.fr/', 'https://www.foxnews.com/', 'https://www.nbcnews.com/']\n",
    "print(\"Websites : \", websites)\n",
    "for topic in topics:\n",
    "    articles = scrape_news(topic[1], websites)\n",
    "\n",
    "\n",
    "print(\"Articles trouvés :\")\n",
    "print(articles.text[re.findall(topics[0] + \" \",articles.text.lower()-50 : re.findall(topics[0] + \" \",articles.text.lower()+50)))\n",
    "print(articles.text[articles.text.lower().find(topics[0] + \" \")-50 : articles.text.lower().find(topics[0])+50])\n",
    "#print(articles.text.lower().find(topics[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
